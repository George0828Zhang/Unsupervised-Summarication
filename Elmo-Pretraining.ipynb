{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import random\n",
    "from preprocessors import BOS, EOS, PAD, UNK\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ELMo import LanguageModel\n",
    "from dataset import *\n",
    "    \n",
    "# class Dataset:\n",
    "#     def __init__(self, filename, vocab):\n",
    "#         self.data = json.load(open(filename, \"r\"))['tokens']\n",
    "#         self.vocab = vocab\n",
    "#         self.pad_idx = vocab[PAD]\n",
    "#         idx = np.argsort([len(x) for x in self.data])[::-1] # descending\n",
    "        \n",
    "#         self.data = [ self.data[i] for i in idx]\n",
    "#         self.size = len(self.data)\n",
    "    \n",
    "#     def tokens_to_ids(self, s):\n",
    "#         return [self.vocab.get(t, self.vocab[UNK]) for t in s]\n",
    "    \n",
    "#     def np_jagged(self, array):\n",
    "#         MAX = max([len(i) for i in array])\n",
    "#         out = [ a + [self.pad_idx]*(MAX-len(a)) if len(a) < MAX else a[:MAX] for a in array ]\n",
    "#         return np.asarray(out, dtype=np.int64)\n",
    "    \n",
    "#     def at(self, i, batch_size):\n",
    "#         fr = i*batch_size\n",
    "#         to = min(fr+batch_size, self.size)\n",
    "#         s = self.data[fr:to] # batch, jagged, 1\n",
    "#         word_ids = self.np_jagged([self.tokens_to_ids(i)[1:] for i in s])\n",
    "#         char_ids = batch_to_ids(s)[:,:-1,:]\n",
    "#         return char_ids, torch.from_numpy(word_ids)\n",
    "    \n",
    "# class Loader(object):\n",
    "#     def __init__(self, dataset, batch_size, shuffle):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "        \n",
    "#         # preprocess\n",
    "#         total = dataset.size // batch_size\n",
    "#         if total * batch_size < dataset.size:\n",
    "#             total += 1\n",
    "        \n",
    "#         self.total = total\n",
    "                    \n",
    "#     def __iter__(self):\n",
    "#         if self.shuffle:            \n",
    "#             r = list(range(self.total))\n",
    "#             random.shuffle(r)\n",
    "#             self.iters = iter(r)\n",
    "#         else:\n",
    "#             self.iters = iter(range(self.total))\n",
    "#         return self\n",
    "    \n",
    "#     def __next__(self):\n",
    "#         return self.next()\n",
    "    \n",
    "#     def next(self):\n",
    "#         index = next(self.iters)\n",
    "#         return self.dataset.at(index, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5c1dafaafd4afcb03bfd2ba6480045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3796361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475691e1ca834cba803a91c6d80b1e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7596), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = json.load(open(\"data-giga/vocab.json\", \"r\"))\n",
    "vocab_size = len(vocab)\n",
    "training_set = PretrainDataset(\"data-giga/train_seq.json\", 50, 50, vocab[PAD]) #train_seq\n",
    "validation_set = PretrainDataset(\"data-giga/valid_seq.json\", 50, 50, vocab[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch_size_inf = 64\n",
    "training_generator = Loader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_generator = Loader(validation_set, batch_size=batch_size_inf, shuffle=False)\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "total_valid = int(math.ceil(validation_set.size / batch_size_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = LanguageModel(vocab).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD]).to(device)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    with torch.no_grad():   \n",
    "        trange = tqdm(validation_generator, total=total_valid)\n",
    "        for src, tgt in trange:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            logits = model(src)\n",
    "            loss = criterion(logits.view(-1, vocab_size), tgt.view(-1))            \n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "    return np.mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 5\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start != 1:\n",
    "    smodel = torch.load(\"trainedELMo/Model\"+str(start-1))\n",
    "    model.load_state_dict(smodel['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(start, epochs+1):\n",
    "    model.train()\n",
    "    print(\"[epoch]\", e)\n",
    "    loss_history = []\n",
    "    trange = tqdm(training_generator, total=total_train)\n",
    "    \n",
    "    for src, tgt in trange:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        logits = model(src)\n",
    "        loss = criterion(logits.view(-1, vocab_size), tgt.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_history.append(loss.item())\n",
    "        trange.set_postfix(**{'loss':'{:.5f}'.format(loss.item())})\n",
    "        \n",
    "    print(\"Epoch train loss:\", np.mean(loss_history))\n",
    "    print(\"Epoch valid loss:\", validation())\n",
    "        \n",
    "    !mkdir -p trained\n",
    "    torch.save({\"model\":model.state_dict(), \"loss\":loss_history}, \"trained/Model\"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444dd4b87f214b3188ef59470e8a6620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=119), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid loss: 2.917541948687129\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch valid loss:\", validation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.025017204176389\n",
      "3.5235224604258963\n",
      "3.4998621888060946\n",
      "3.4962652858272785\n",
      "3.4944170089836244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "for i in range(5):\n",
    "    s = torch.load(\"trainedELMo/Model\"+str(i+1))\n",
    "    mean = np.mean(s['loss'])\n",
    "    print(mean)\n",
    "    losses.append(mean)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
