{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import *\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/tmp2/b05902064/data-giga/\"\n",
    "train_path = data_dir + \"train_seq.json\"\n",
    "vocab_path = data_dir + \"vocab.json\"\n",
    "lm_path = \"trainedELMo\"\n",
    "device = torch.device(\"cuda:1\")\n",
    "vocab = json.load(open(vocab_path))\n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = torch.load(\"/tmp2/b05902064/pretrained_ELMo\")\n",
    "LM = torch.load(\"/tmp2/b05902064/preload_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextMatcher(\n",
       "  (LM): LanguageModel(\n",
       "    (elmo): Elmo(\n",
       "      (_elmo_lstm): _ElmoBiLm(\n",
       "        (_token_embedder): _ElmoCharacterEncoder(\n",
       "          (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "          (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "          (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "          (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "          (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "          (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "          (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "          (_highways): Highway(\n",
       "            (_layers): ModuleList(\n",
       "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "              (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (_word_embedding): Embedding()\n",
       "        (_elmo_lstm): ElmoLstm(\n",
       "          (forward_layer_0): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (backward_layer_0): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (forward_layer_1): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (backward_layer_1): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_dropout): Dropout(p=0.5, inplace=False)\n",
       "      (scalar_mix_0): ScalarMix(\n",
       "        (scalar_parameters): ParameterList(\n",
       "            (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "            (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "            (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (project): Linear(in_features=1024, out_features=50000, bias=True)\n",
       "    (CE): CrossEntropyLoss()\n",
       "  )\n",
       "  (pretrained_elmo): Elmo(\n",
       "    (_elmo_lstm): _ElmoBiLm(\n",
       "      (_token_embedder): _ElmoCharacterEncoder(\n",
       "        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "        (_highways): Highway(\n",
       "          (_layers): ModuleList(\n",
       "            (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "            (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_word_embedding): Embedding()\n",
       "      (_elmo_lstm): ElmoLstm(\n",
       "        (forward_layer_0): LstmCellWithProjection(\n",
       "          (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "          (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "          (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        )\n",
       "        (backward_layer_0): LstmCellWithProjection(\n",
       "          (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "          (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "          (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        )\n",
       "        (forward_layer_1): LstmCellWithProjection(\n",
       "          (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "          (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "          (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        )\n",
       "        (backward_layer_1): LstmCellWithProjection(\n",
       "          (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "          (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "          (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (_dropout): Dropout(p=0.5, inplace=False)\n",
       "    (scalar_mix_0): ScalarMix(\n",
       "      (scalar_parameters): ParameterList(\n",
       "          (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "          (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "          (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ContextMatcher(vocab, elmo, LM)\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building candidate mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3c729358984c7eba22d214d48b16cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([50000, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"building candidate mapping\")\n",
    "# (vocab, emb)\n",
    "vocab_size = len(vocab)\n",
    "batch_size = 512\n",
    "embeddings = []\n",
    "total = int(math.ceil(vocab_size/batch_size))\n",
    "progress = tqdm(range(total), total=total)\n",
    "for i in progress:\n",
    "    fr = i*batch_size\n",
    "    to = min(fr+batch_size, vocab_size)\n",
    "    indices = torch.arange(fr, to).unsqueeze(1)\n",
    "    embeddings.append(m.embed(indices.to(device)) )\n",
    "\n",
    "# (n_batch, batch_size)\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "print(embeddings.shape)\n",
    "# (vocab, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 1024])\n"
     ]
    }
   ],
   "source": [
    "embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "embeddings = embeddings.squeeze()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = torch.matmul(embeddings, embeddings.transpose(0, 1))\n",
    "# print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = torch.argsort(embeddings, dim=-1, descending=True)\n",
    "# embeddings = embeddings[:, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd098ff0c7e0403d9951ad7b7531764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in tqdm(range(50000), total=50000):\n",
    "    # (50000, 1024)\n",
    "    cur = embeddings[i:i+1] #(1, 1024)\n",
    "    scores = torch.matmul(cur, embeddings.transpose(0, 1)) #(1, 50000)\n",
    "    candidates = torch.argsort(scores, dim=-1, descending=True) #(1, 50000)\n",
    "    output.append(candidates[0,:50].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.stack(output, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, \"/tmp2/b05902064/candidate_mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
